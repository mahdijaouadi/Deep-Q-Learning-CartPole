{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE=32\n",
    "BUFFER_SIZE=50000\n",
    "MIN_REPLAY_SIZE=1800\n",
    "EPSILON_START=1.0\n",
    "EPSILON_END=0.02\n",
    "EPSILON_DECAY=10000\n",
    "TARGET_UPDATE_FREQ=1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        in_features=int(np.prod(env.observation_space.shape))\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(in_features,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,env.action_space.n)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    def act(self,obs):\n",
    "        obs_t=torch.as_tensor(obs,dtype=torch.float32).to(device)\n",
    "        q_values=self(obs_t.unsqueeze(0))\n",
    "        max_q_index=torch.argmax(q_values,dim=1)[0]\n",
    "        action=max_q_index.detach().item()\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v0')\n",
    "replay_buffer=deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer=[]\n",
    "epsilon_degradation=[]\n",
    "episode_reward=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda'\n",
    "online_net=Network(env).to(device)\n",
    "target_net=Network(env).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playing random games to fill the reward buffer\n",
    "obs=env.reset()[0]\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action=env.action_space.sample()\n",
    "    new_obs,rew,done,info,_=env.step(action)\n",
    "    transistion=(obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transistion)\n",
    "    obs=new_obs\n",
    "    if done:\n",
    "        obs=env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(transistions):\n",
    "    for i in range(0,len(transistions)-BATCH_SIZE,BATCH_SIZE):\n",
    "        transistions0=transistions[i:i+BATCH_SIZE]\n",
    "        obses=np.asarray([t[0] for t in transistions0])\n",
    "        actions=np.asarray([t[1] for t in transistions0])\n",
    "        rews=np.asarray([t[2] for t in transistions0])\n",
    "        dones=np.asarray([t[3] for t in transistions0])\n",
    "        new_obses=np.asarray([t[4] for t in transistions0])\n",
    "        obses_t=torch.as_tensor(obses,dtype=torch.float32)\n",
    "        actions_t=torch.as_tensor(actions,dtype=torch.int64).unsqueeze(-1)\n",
    "        rews_t=torch.as_tensor(rews,dtype=torch.float32).unsqueeze(-1)\n",
    "        dones_t=torch.as_tensor(dones,dtype=torch.float32).unsqueeze(-1)\n",
    "        new_obses_t=torch.as_tensor(new_obses,dtype=torch.float32)\n",
    "        target_q_values=target_net(new_obses_t.to(device)).to('cpu')\n",
    "        max_target_q_values=target_q_values.max(dim=1,keepdim=True)[0]\n",
    "        targets=(rews_t+GAMMA*(1-dones_t)*max_target_q_values).to(device)\n",
    "\n",
    "        q_values=(online_net(obses_t.to(device))).to('cpu')\n",
    "        action_q_values=(torch.gather(input=q_values,dim=1,index=actions_t)).to(device)\n",
    "        loss=nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(online_net.parameters(), lr=5e-4)\n",
    "obs=env.reset()[0]\n",
    "for step in itertools.count():\n",
    "    epsilon=np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample=random.random()\n",
    "    if rnd_sample<=epsilon:\n",
    "        action=env.action_space.sample()\n",
    "    else:\n",
    "        action=online_net.act(obs)\n",
    "    new_obs,rew,done,info,_=env.step(action)\n",
    "    transistion=(obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transistion)\n",
    "    obs=new_obs\n",
    "    episode_reward+=rew\n",
    "    if done:\n",
    "        obs=env.reset()[0]\n",
    "        rew_buffer.append(episode_reward)\n",
    "        epsilon_degradation.append(epsilon)\n",
    "        episode_reward=0.0\n",
    "\n",
    "    if step%100==0:\n",
    "        transistions = list(replay_buffer)\n",
    "        train_model(transistions)\n",
    "    if step % TARGET_UPDATE_FREQ==0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    if step % 100==0:\n",
    "        print()\n",
    "        print('step',step)\n",
    "        print('avg reward',np.mean(rew_buffer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.scatter(x=range(len(rew_buffer)),y=rew_buffer)\n",
    "ax2.scatter(x=range(len(epsilon_degradation)),y=epsilon_degradation)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
